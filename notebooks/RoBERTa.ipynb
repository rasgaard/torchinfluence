{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.func import functional_call, grad, vmap\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "from torchinfluence.methods import GradientSimilarity\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"aychang/roberta-base-imdb\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"aychang/roberta-base-imdb\")\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, split: str = \"train\"):\n",
    "        self.dataset = dataset[split]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "        return dict(encoding), torch.tensor(label).long()\n",
    "\n",
    "    def decode(self, idx):\n",
    "        input_ids = self[idx][0][\"input_ids\"][0]\n",
    "        return self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb, tokenizer, split=\"train\")\n",
    "test_dataset = IMDBDataset(imdb, tokenizer, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_subset = [\"roberta.embeddings.word_embeddings.weight\"]\n",
    "\n",
    "params = dict(model.named_parameters())\n",
    "params = {name: param for name, param in params.items() if name in parameter_subset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFSeqClfGradientSimilarity(GradientSimilarity):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        loss_fn: Callable,\n",
    "        device: str = \"cpu\",\n",
    "        parameter_subset: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(model, loss_fn, device, parameter_subset)\n",
    "\n",
    "    def _compute_loss(self, params, inputs, targets):\n",
    "        prediction = functional_call(self.model, params, (*inputs,))\n",
    "        prediction = prediction.logits\n",
    "\n",
    "        return self.loss_fn(prediction, targets)\n",
    "\n",
    "    def dataset_gradients(self, inputs: Union[str, torch.Tensor], targets: torch.Tensor):\n",
    "        inputs_shape = inputs[\"input_ids\"].shape\n",
    "\n",
    "        if (len(inputs_shape) == 3) and (inputs_shape[1] == 1):\n",
    "            inputs = {k: v.squeeze(1) for k, v in inputs.items()}\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "\n",
    "        compute_grads = vmap(grad(self._compute_loss), in_dims=(None, 0, 0), chunk_size=self.chunk_size)\n",
    "        grads = compute_grads(\n",
    "            self.params,\n",
    "            (\n",
    "                input_ids.unsqueeze(1),\n",
    "                attention_mask.unsqueeze(1),\n",
    "            ),\n",
    "            targets.unsqueeze(1),\n",
    "        )\n",
    "        grads = torch.hstack([g.flatten() for g in list(grads.values())]).reshape(inputs_shape[0], -1)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "gradsim = HFSeqClfGradientSimilarity(model, F.cross_entropy, device=device, parameter_subset=parameter_subset)\n",
    "\n",
    "n_train, n_test = 250, 1\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train_idxs = np.random.choice(np.arange(len(train_dataset)), n_train, replace=False).tolist()\n",
    "test_idx = np.random.choice(np.arange(len(test_dataset)), n_test, replace=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25ebcbc11e04886adfdd8b373c20a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = gradsim.score(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    subset_ids={\"test\": test_idx, \"train\": train_idxs},\n",
    "    normalize=True,\n",
    "    chunk_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 250])\n"
     ]
    }
   ],
   "source": [
    "print(scores.shape)  # (n_test, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST EXAMPLE\n",
      "I remember disliking this movie the 1st time I saw it, but it has grown on me. I love the costumes and poses the actors make, the humor, the cinematography, the soundtrack. The scenes are very rich, and it moves very quickly. Every time I watch it, there is something new that catches my eye. Aaliyah as Akasha is probably the only thing that ruins it, but not enough.<br /><br />Also, the Lestat in this movie IS different, it is not the same character. You can see that the character Armand has been given Lestat-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST EXAMPLE\")\n",
    "print(test_dataset.decode(test_idx))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOST INFLUENTIAL TRAINING EXAMPLE\n",
      "When will the hurting stop? I never want to see another version of a Christmas Carol again. They keep on making movies with the same story, falling over each other in trying to make the movie better then the rest, but sadly fail to do so, as this is not a good story. Moralistic, old-fashioned, conservative happy-thinking. As if people learn. The numerous different versions of this film prove that we donÂ´t.\n"
     ]
    }
   ],
   "source": [
    "print(\"MOST INFLUENTIAL TRAINING EXAMPLE\")\n",
    "print(train_dataset.decode(scores[0].sort(descending=True).indices[0].item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
