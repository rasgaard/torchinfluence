{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.func import functional_call, grad, vmap\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "from torchinfluence.methods import GradientSimilarity\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"aychang/roberta-base-imdb\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"aychang/roberta-base-imdb\")\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, split: str = \"train\"):\n",
    "        self.dataset = dataset[split]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx][\"text\"]\n",
    "        label = self.dataset[idx][\"label\"]\n",
    "\n",
    "        encoding = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "        return dict(encoding), torch.tensor(label).long()\n",
    "\n",
    "    def decode(self, idx):\n",
    "        input_ids = self[idx][0][\"input_ids\"][0]\n",
    "        return self.tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb, tokenizer, split=\"train\")\n",
    "test_dataset = IMDBDataset(imdb, tokenizer, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_subset = [\"roberta.embeddings.word_embeddings.weight\"]\n",
    "\n",
    "params = dict(model.named_parameters())\n",
    "params = {name: param for name, param in params.items() if name in parameter_subset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Optional, Union\n",
    "\n",
    "\n",
    "class HFSeqClfGradientSimilarity(GradientSimilarity):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        loss_fn: Callable,\n",
    "        device: str = \"cpu\",\n",
    "        parameter_subset: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__(model, loss_fn, device, parameter_subset)\n",
    "\n",
    "    def _compute_loss(self, params, inputs, targets):\n",
    "        prediction = functional_call(self.model, params, (*inputs,))\n",
    "        prediction = prediction.logits\n",
    "\n",
    "        return self.loss_fn(prediction, targets)\n",
    "\n",
    "    def dataset_gradients(self, inputs: Union[str, torch.Tensor], targets: torch.Tensor):\n",
    "        inputs_shape = inputs[\"input_ids\"].shape\n",
    "\n",
    "        if (len(inputs_shape) == 3) and (inputs_shape[1] == 1):\n",
    "            inputs = {k: v.squeeze(1) for k, v in inputs.items()}\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        compute_grads = vmap(grad(self._compute_loss), in_dims=(None, 0, 0), chunk_size=self.chunk_size)\n",
    "        grads = compute_grads(\n",
    "            self.params,\n",
    "            (\n",
    "                input_ids.unsqueeze(1),\n",
    "                attention_mask.unsqueeze(1),\n",
    "            ),\n",
    "            targets.unsqueeze(1),\n",
    "        )\n",
    "        grads = torch.hstack([g.flatten() for g in list(grads.values())]).reshape(inputs_shape[0], -1)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradsim = HFSeqClfGradientSimilarity(model, F.cross_entropy)\n",
    "\n",
    "train_idxs = np.random.choice(np.arange(len(train_dataset)), 100, replace=False).tolist()\n",
    "test_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e147b3e6cc944ac93d70049584f8307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = gradsim.score(\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    subset_ids={\"test\": [test_idx], \"train\": train_idxs},\n",
    "    normalize=True,\n",
    "    chunk_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape  # (n_test, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.decode(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I very much looked forward to this movie. Its a good family movie; however, if Michael Landon Jr.'s editing team did a better job of editing, the movie would be much better. Too many scenes out of context. I do hope there is another movie from the series, they're all very good. But, if another one is made, I beg them to take better care at editing. This story was all over the place and didn't seem to have a center. Which is unfortunate because the other movies of the series were great. I enjoy the story of Willie and Missy; they're both great role\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.decode(scores[0].sort(descending=True).indices[0].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
